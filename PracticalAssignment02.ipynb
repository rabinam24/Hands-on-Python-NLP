{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15xGok1awaxNfSp_IhMJs58_ZfT3IZuoN",
      "authorship_tag": "ABX9TyM692t4I+8+Ehf7bD/qjeJO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabinam24/Hands-on-Python-NLP/blob/main/PracticalAssignment02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fUQ2vBD32v7",
        "outputId": "6b56c415-8af1-4c94-9551-d240348b3c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_path=\"/content/drive/MyDrive/zomato_reviews.csv\"\n",
        "df=pd.read_csv(df_path)\n"
      ],
      "metadata": {
        "id": "2yyNr4q340qk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EMCPen9S47uh",
        "outputId": "be1bedd0-8dc5-4122-ae78-2a14113c40d4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              Review sentiment\n",
              "0  Virat Kohli did a great thing to open his rest...  positive\n",
              "1  This place have some really heathy options to ...  positive\n",
              "2  Aerocity is the most finest place in Delhi for...  positive\n",
              "3  Yesterday evening there was small team lunch ,...  positive\n",
              "4  I find aerocity to be the best place in delhi ...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-754a1db1-bec0-4db8-8da4-e173699c40a2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Virat Kohli did a great thing to open his rest...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This place have some really heathy options to ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Aerocity is the most finest place in Delhi for...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Yesterday evening there was small team lunch ,...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I find aerocity to be the best place in delhi ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-754a1db1-bec0-4db8-8da4-e173699c40a2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-754a1db1-bec0-4db8-8da4-e173699c40a2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-754a1db1-bec0-4db8-8da4-e173699c40a2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-adbc7795-c87e-4b42-b2ab-e41ddf939883\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-adbc7795-c87e-4b42-b2ab-e41ddf939883')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-adbc7795-c87e-4b42-b2ab-e41ddf939883 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1596,\n  \"fields\": [\n    {\n      \"column\": \"Review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 981,\n        \"samples\": [\n          \"This outlet of barbecue nation remains most crowded out of all. This place has different vibe unlike the other outlets. Starters were good as usual but main course again failed to please me. Dessert option was very less this time, Which was an issue for me. The staff are very courteous and friendly. Overall nice experience.\",\n          \"The first opinion you make when you enter is their ambiance, this place is not that huge!, This is one of the most peacefully decorated and pleasant resto bar I've ever been, it has its own brewery and it's famous for it's compact looks. So many items and everything looked tempting. One cannot eat everything unless you have a good appetite. Good crowd as well.\",\n          \"This market is famous because of this outlet, I liked galauti kebabs, tangri kebab and tanddori kebabs with laccha paratha. I will surely suggest you to visit it once it's totally worth it\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = pd.Series(df.Review.tolist()).astype(str)"
      ],
      "metadata": {
        "id": "lqpFRzQ-5pas"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMukB8067Gxq",
        "outputId": "e2caa9d1-2053-4dfe-fbc7-55f4b463fa4b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Virat Kohli did a great thing to open his rest...\n",
              "1       This place have some really heathy options to ...\n",
              "2       Aerocity is the most finest place in Delhi for...\n",
              "3       Yesterday evening there was small team lunch ,...\n",
              "4       I find aerocity to be the best place in delhi ...\n",
              "                              ...                        \n",
              "1591    || DESI LANE || So we were at alipore's most h...\n",
              "1592    \"Desi Lane\" is one of the most trending place ...\n",
              "1593    One of the cool and pocket pinch restaurant at...\n",
              "1594    \"DESI LANE\" one of the best places in town and...\n",
              "1595    Looking for good place for lunch but dont wann...\n",
              "Length: 1596, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "Pdb_js_l7JFn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "q8bUI3jl7Ppe"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "clay2xYI7TRR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "JJiRMH7R7Wxw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "Jb0xpoBJ7aU2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "common_dot_words = ['U.S.A', 'Mr.', 'Mrs.', 'D.C.']"
      ],
      "metadata": {
        "id": "8KpI4gvc7euj"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "corpus_with_lemmatization = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None, lemmatization = True, remove_stopwords = True)"
      ],
      "metadata": {
        "id": "ebJooLS67h2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_with_stemming = preprocess(corpus, keep_list=common_dot_words, stemming=True, stem_type=\"snowball\", lemmatization=False, remove_stopwords=True)\n"
      ],
      "metadata": {
        "id": "zEw12sev7mjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original string: \", corpus[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNmHHR1N85qD",
        "outputId": "0a71eef7-9a6f-4f2d-bc04-14b81c827662"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original string:  Virat Kohli did a great thing to open his restaurant in an exquisite place of Delhi. Wide range of food with lots and lots of options on drinks. Courteous staff with a quick response on anything.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"String after lemmatiization: \", corpus_with_lemmatization[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLfVpOe29XW1",
        "outputId": "a3d04b2d-8154-4b27-f21e-b7b1f8b2e420"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String after lemmatiization:  virat kohli great thing open restaurant exquisite place delhi wide range food lot lot options drink courteous staff quick response anything\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"String after stemming: \", corpus_with_stemming[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRdmC1nX9aNI",
        "outputId": "f31bc2e8-6109-4a0b-9fc9-59ff22593918"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "String after stemming:  virat koh great thing open restaur exquisit place delhi wide rang food lot lot option drink courteous staff quick respons anyth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "cYxV9opf9dZw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plurals = ['caresses', 'flies', 'dies', 'mules', 'died', 'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "           'siezing', 'itemization', 'traditional', 'reference', 'colonizer', 'plotted', 'having', 'generously']"
      ],
      "metadata": {
        "id": "yRql2Hdd94ph"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "singles = [stemmer.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBDmyXqN9iS5",
        "outputId": "68594325-8bc1-4443-a9b7-2d13c63b0d47"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have gener\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "print(SnowballStemmer.languages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ic7QeRrr9lIW",
        "outputId": "c574e5f2-570d-481f-c01f-a5c7013302dd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'hungarian', 'italian', 'norwegian', 'porter', 'portuguese', 'romanian', 'russian', 'spanish', 'swedish')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "singles = [stemmer2.stem(plural) for plural in plurals]\n",
        "print(' '.join(singles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJySDzhu9_XE",
        "outputId": "0ed35c66-92f0-421f-a778-8007efae1bb5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "caress fli die mule die agre own humbl size meet state siez item tradit refer colon plot have generous\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObTPkYBE-63X",
        "outputId": "00a4186f-50a9-46b0-884c-ac4db452d2e8"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "token_list = s.split()\n",
        "print(\"The tokens are: \", token_list)\n",
        "lemmatized_output = ' '.join([lemmatizer.lemmatize(token) for token in token_list])\n",
        "print(\"The lemmatized output is: \", lemmatized_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RwUHm2F--vA",
        "outputId": "a664530d-3d9d-4c00-c505-0cae3ab8f91e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tokens are:  ['We', 'are', 'putting', 'in', 'efforts', 'to', 'enhance', 'our', 'understanding', 'of', 'Lemmatization']\n",
            "The lemmatized output is:  We are putting in effort to enhance our understanding of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_tags = nltk.pos_tag(token_list)\n",
        "pos_tags"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl71HsrK_BhU",
        "outputId": "a3b7ba11-893f-4079-b4cf-33f3cbcd58fb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('We', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('putting', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('efforts', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('enhance', 'VB'),\n",
              " ('our', 'PRP$'),\n",
              " ('understanding', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('Lemmatization', 'NN')]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "##This is a common method which is widely used across the NLP community of practitioners and readers\n",
        "\n",
        "def get_part_of_speech_tags(token):\n",
        "\n",
        "    \"\"\"Maps POS tags to first character lemmatize() accepts.\n",
        "    We are focussing on Verbs, Nouns, Adjectives and Adverbs here.\"\"\"\n",
        "\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    tag = nltk.pos_tag([token])[0][1][0].upper()\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)"
      ],
      "metadata": {
        "id": "GahIY5EG_EEI"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_output_with_POS_information = [lemmatizer.lemmatize(token, get_part_of_speech_tags(token)) for token in token_list]\n",
        "print(' '.join(lemmatized_output_with_POS_information))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhb93A7s_HUS",
        "outputId": "4d0227c5-51e8-4c1c-c0c5-916abf6e9e5f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We be put in effort to enhance our understand of Lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer2 = SnowballStemmer(language='english')\n",
        "stemmed_sentence = [stemmer2.stem(token) for token in token_list]\n",
        "print(' '.join(stemmed_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux-awfuJ_Ki8",
        "outputId": "2f3e1cec-6c25-419a-f9fa-53b001fbf926"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we are put in effort to enhanc our understand of lemmat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model from spaCy\n",
        "nlp = spacy.load('en_core_web_sm')  # Change 'en' to 'en_core_web_sm'\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(\"We are putting in efforts to enhance our understanding of Lemmatization\")\n",
        "\n",
        "# Extract the lemmas of each token and join them into a string\n",
        "result = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "# Print or use the resulting string\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgstoEg0_Nr-",
        "outputId": "87f2b976-095b-44f5-cf4f-5bd24462f914"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we be put in effort to enhance our understanding of lemmatization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop = set(stopwords.words('english'))\n",
        "\", \".join(stop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "4IwYLbMs_Q_b",
        "outputId": "c5ada753-4bbb-49fd-d2c2-f6b39ccc6284"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"him, in, to, are, a, or, couldn, those, all, when, own, has, ll, mustn, wouldn, same, have, y, its, weren, be, her, this, some, not, wasn, down, o, yours, his, did, now, until, will, d, didn't, more, haven't, aren't, i, mightn't, shouldn, were, needn, shan, any, doesn't, ours, ain, out, themselves, isn, an, than, there, s, it, mightn, you'll, shan't, hers, how, is, above, shouldn't, you, does, about, no, of, but, ourselves, am, below, herself, she, here, each, don, won, your, few, once, most, my, been, by, having, they, so, isn't, only, was, over, haven, at, itself, yourselves, weren't, do, that, t, it's, against, didn, these, don't, you've, being, them, through, with, our, as, other, why, just, what, and, such, needn't, very, aren, hasn't, doing, for, too, should've, the, again, should, whom, between, himself, who, that'll, off, ve, you're, both, before, from, because, hadn, on, couldn't, up, ma, yourself, had, re, you'd, m, hadn't, she's, myself, under, further, their, nor, theirs, if, hasn, can, mustn't, wasn't, after, into, then, doesn, which, wouldn't, while, he, we, during, won't, me, where\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "sentence = \"how are we putting in efforts to enhance our understanding of Lemmatization\"\n",
        "\n",
        "for word in wh_words:\n",
        "    stop.remove(word)\n",
        "\n",
        "sentence_after_stopword_removal = [token for token in sentence.split() if token not in stop]\n",
        "\" \".join(sentence_after_stopword_removal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ttcXMfNg_Trv",
        "outputId": "a22096eb-8d26-4593-d46d-f9d9bf2e8f00"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'how putting efforts enhance understanding Lemmatization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"We are putting in efforts to enhance our understanding of Lemmatization\"\n",
        "s = s.lower()\n",
        "s"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Y5AwK6U3_nRt",
        "outputId": "338a1fcc-0b6e-4b55-b82a-9faf9170e773"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'we are putting in efforts to enhance our understanding of lemmatization'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "[\" \".join(token) for token in bigrams]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8zIw8MM_r7s",
        "outputId": "c9e53b38-8dd3-499b-c567-b54542156bf0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language',\n",
              " 'Language Processing',\n",
              " 'Processing is',\n",
              " 'is the',\n",
              " 'the way',\n",
              " 'way to',\n",
              " 'to go']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = s.split()\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "[\" \".join(token) for token in trigrams]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPFp1EgA_vV5",
        "outputId": "48763244-d651-4607-e7e0-11fe2055f085"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural Language Processing',\n",
              " 'Language Processing is',\n",
              " 'Processing is the',\n",
              " 'is the way',\n",
              " 'the way to',\n",
              " 'way to go']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"Natural Language Processing is the way to go\"\n",
        "tokens = set(s.split())\n",
        "vocabulary = sorted(tokens)\n",
        "vocabulary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLb68EVJ_yMO",
        "outputId": "3081e9ae-c43a-4ea5-d719-e2f8549ced81"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Language', 'Natural', 'Processing', 'go', 'is', 'the', 'to', 'way']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "html = \"<!DOCTYPE html><html><body><h1>My First Heading</h1><p>My first paragraph.</p></body></html>\"\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(html)\n",
        "text = soup.get_text()\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riEukti3_05P",
        "outputId": "9f6bb57a-49b2-49ab-b78d-06ae1388117e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My First HeadingMy first paragraph.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The capital of China is Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBM5tilt_3yc",
        "outputId": "610444ef-b4fa-46fd-a7fa-c037ebedb9ea"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The', 'capital', 'of', 'China', 'is', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"China's capital is Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW9jmpvgAAG1",
        "outputId": "70764162-73b5-4de9-9126-8b97b04f3dbf"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"China's\", 'capital', 'is', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Beijing is where we'll go\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGfU2qpOAC7x",
        "outputId": "27085554-fe44-4a33-bede-df45124b2c72"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'where', \"we'll\", 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I'm going to travel to Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGBdbIToAF3P",
        "outputId": "08cbb380-d9fa-479e-8e32-483f9eb0fc58"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\", 'going', 'to', 'travel', 'to', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Most of the times umm I travel\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hkpIEswAJGg",
        "outputId": "0d1bb95d-d05a-43b0-8b05-88dea7844006"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Most', 'of', 'the', 'times', 'umm', 'I', 'travel']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Let's travel to Hong Kong from Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgjiU0IZALzw",
        "outputId": "18f64f3c-1c95-42ad-d84f-55bc6cdb38ef"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'travel', 'to', 'Hong', 'Kong', 'from', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"A friend is pursuing his M.S from Beijing\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZB7dKYUjAOF9",
        "outputId": "838d50eb-9640-4672-f6f4-c8b2ba0e9b1c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A', 'friend', 'is', 'pursuing', 'his', 'M.S', 'from', 'Beijing']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Beijing is a cool place!!! :-P <3 #Awesome\"\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yTF52EMAQq0",
        "outputId": "c1d35209-df2f-4e93-8ba6-a4c94e2bad3b"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Beijing', 'is', 'a', 'cool', 'place!!!', ':-P', '<3', '#Awesome']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\"\n",
        "tokenizer = RegexpTokenizer('\\w+|\\$[\\d\\.]+|\\S+')\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtZBAWVZATHx",
        "outputId": "ff650a16-ad5b-473c-94a2-36773536d1e8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$3000.0',\n",
              " '-',\n",
              " '$8000.0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import BlanklineTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n\\n I want a book as well\"\n",
        "tokenizer = BlanklineTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXzywzcMAWI2",
        "outputId": "ce1702a2-5023-4cd4-9d4b-99653b452e24"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.',\n",
              " 'I want a book as well']"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "s = \"A Rolex watch costs in the range of $3000.0 - $8000.0 in USA.\\n I want a book as well\"\n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUjqhfImAYoT",
        "outputId": "d90cea8a-b8bb-4c9d-ae59-2ebea8ff0634"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'costs',\n",
              " 'in',\n",
              " 'the',\n",
              " 'range',\n",
              " 'of',\n",
              " '$',\n",
              " '3000',\n",
              " '.',\n",
              " '0',\n",
              " '-',\n",
              " '$',\n",
              " '8000',\n",
              " '.',\n",
              " '0',\n",
              " 'in',\n",
              " 'USA',\n",
              " '.',\n",
              " 'I',\n",
              " 'want',\n",
              " 'a',\n",
              " 'book',\n",
              " 'as',\n",
              " 'well']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "s = \"I'm going to buy a Rolex watch which doesn't cost more than $3000.0\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o93TBdnbAbfe",
        "outputId": "c570afb8-47ce-423c-b8df-c0856d17fdf1"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " \"'m\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolex',\n",
              " 'watch',\n",
              " 'which',\n",
              " 'does',\n",
              " \"n't\",\n",
              " 'cost',\n",
              " 'more',\n",
              " 'than',\n",
              " '$',\n",
              " '3000.0']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer()\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65a8G1GAAeUL",
        "outputId": "eb3ab8b8-6638-4857-81d9-ccfe94066b29"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['@amankedia',\n",
              " \"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxxxxxxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "s = \"@amankedia I'm going to buy a Rolexxxxxxxx watch!!! :-D #happiness #rolex <3\"\n",
        "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
        "tokenizer.tokenize(s)"
      ],
      "metadata": {
        "id": "gMamw6RdAg6a",
        "outputId": "475f40ef-8ff0-49e3-c813-ae48c9d58006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"I'm\",\n",
              " 'going',\n",
              " 'to',\n",
              " 'buy',\n",
              " 'a',\n",
              " 'Rolexxx',\n",
              " 'watch',\n",
              " '!',\n",
              " '!',\n",
              " '!',\n",
              " ':-D',\n",
              " '#happiness',\n",
              " '#rolex',\n",
              " '<3']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eH3DahxKAjQG"
      },
      "execution_count": 50,
      "outputs": []
    }
  ]
}