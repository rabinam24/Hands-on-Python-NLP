{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1K4D4ZDDdYD6Icy_M3D_s-mlkYGKLx9xq",
      "authorship_tag": "ABX9TyPcUymLR2+x0RlgAylywdyA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rabinam24/Hands-on-Python-NLP/blob/main/PracticalAssignment03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsCxWaOAMct6",
        "outputId": "608a2117-ba78-4263-a8d2-8e0ea74f9a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'computers': 2, 'analyze': 1, 'text': 7, 'using': 8, 'vectors': 9, 'matrices': 5, 'process': 6, 'massive': 4, 'amounts': 0, 'data': 3}\n",
            "[[0 1 1 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 1 0 0 1 1]\n",
            " [1 0 1 1 1 0 1 1 0 0]]\n"
          ]
        }
      ],
      "source": [
        "#Matrices\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "x= (\"Computers can analyze text\",\"They do it using vectors and matrices\",\"Computers can process massive amounts of text data\")\n",
        "vectorizer= CountVectorizer(stop_words='english')\n",
        "x_vec= vectorizer.fit_transform(x)\n",
        "print(vectorizer.vocabulary_)\n",
        "print(x_vec.todense())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploring the Bag_of_Words Architecture"
      ],
      "metadata": {
        "id": "tMEULgxwN8P2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv49du8CNzp4",
        "outputId": "7af45b37-7b2a-4cab-f950-d741aeef20ff"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"We are reading about Natural Language Processing Here\",\n",
        "\"Natural Language Processing making computers comprehend language data\",\n",
        "\"The field of Natural Language Processing is evolving everyday\"]\n"
      ],
      "metadata": {
        "id": "Yp9qM5HiOnDl"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus= pd.Series(sentences)\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVMnBu4qO0Vr",
        "outputId": "8026f2db-45de-4d5c-f32a-06b86b8b9eb2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    We are reading about Natural Language Processi...\n",
              "1    Natural Language Processing making computers c...\n",
              "2    The field of Natural Language Processing is ev...\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_clean(corpus, keep_list):\n",
        "    '''\n",
        "    Purpose : Function to keep only alphabets, digits and certain words (punctuations, qmarks, tabs etc. removed)\n",
        "\n",
        "    Input : Takes a text corpus, 'corpus' to be cleaned along with a list of words, 'keep_list', which have to be retained\n",
        "            even after the cleaning process\n",
        "\n",
        "    Output : Returns the cleaned text corpus\n",
        "\n",
        "    '''\n",
        "    cleaned_corpus = pd.Series()\n",
        "    for row in corpus:\n",
        "        qs = []\n",
        "        for word in row.split():\n",
        "            if word not in keep_list:\n",
        "                p1 = re.sub(pattern='[^a-zA-Z0-9]',repl=' ',string=word)\n",
        "                p1 = p1.lower()\n",
        "                qs.append(p1)\n",
        "            else : qs.append(word)\n",
        "        cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
        "    return cleaned_corpus"
      ],
      "metadata": {
        "id": "CxtzqXUWO48U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwords_removal(corpus):\n",
        "    wh_words = ['who', 'what', 'when', 'why', 'how', 'which', 'where', 'whom']\n",
        "    stop = set(stopwords.words('english'))\n",
        "    for word in wh_words:\n",
        "        stop.remove(word)\n",
        "    corpus = [[x for x in x.split() if x not in stop] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "0MKMcVrdPVuo"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize(corpus):\n",
        "    lem = WordNetLemmatizer()\n",
        "    corpus = [[lem.lemmatize(x, pos = 'v') for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "33cs7mZRgL1l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem(corpus, stem_type = None):\n",
        "    if stem_type == 'snowball':\n",
        "        stemmer = SnowballStemmer(language = 'english')\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    else :\n",
        "        stemmer = PorterStemmer()\n",
        "        corpus = [[stemmer.stem(x) for x in x] for x in corpus]\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "s-tPtS3hgOD1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, keep_list, cleaning = True, stemming = False, stem_type = None, lemmatization = False, remove_stopwords = True):\n",
        "    '''\n",
        "    Purpose : Function to perform all pre-processing tasks (cleaning, stemming, lemmatization, stopwords removal etc.)\n",
        "\n",
        "    Input :\n",
        "    'corpus' - Text corpus on which pre-processing tasks will be performed\n",
        "    'keep_list' - List of words to be retained during cleaning process\n",
        "    'cleaning', 'stemming', 'lemmatization', 'remove_stopwords' - Boolean variables indicating whether a particular task should\n",
        "                                                                  be performed or not\n",
        "    'stem_type' - Choose between Porter stemmer or Snowball(Porter2) stemmer. Default is \"None\", which corresponds to Porter\n",
        "                  Stemmer. 'snowball' corresponds to Snowball Stemmer\n",
        "\n",
        "    Note : Either stemming or lemmatization should be used. There's no benefit of using both of them together\n",
        "\n",
        "    Output : Returns the processed text corpus\n",
        "\n",
        "    '''\n",
        "\n",
        "    if cleaning == True:\n",
        "        corpus = text_clean(corpus, keep_list)\n",
        "\n",
        "    if remove_stopwords == True:\n",
        "        corpus = stopwords_removal(corpus)\n",
        "    else :\n",
        "        corpus = [[x for x in x.split()] for x in corpus]\n",
        "\n",
        "    if lemmatization == True:\n",
        "        corpus = lemmatize(corpus)\n",
        "\n",
        "\n",
        "    if stemming == True:\n",
        "        corpus = stem(corpus, stem_type)\n",
        "\n",
        "    corpus = [' '.join(x) for x in corpus]\n",
        "\n",
        "    return corpus"
      ],
      "metadata": {
        "id": "1Gq1vf79gQLR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "common_dot_words = ['U.S.', 'Mr.', 'Mrs.', 'D.C.']\n"
      ],
      "metadata": {
        "id": "HrlH-XsYgUUk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = common_dot_words, stemming = False, stem_type = None,\n",
        "                                lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnS_Rp2CgWYf",
        "outputId": "88796233-6e8a-4297-ca50-02051a0e921a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-a7d5d0d93ac1>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-12-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-12-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-12-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "set_of_words = set()\n",
        "for sentence in preprocessed_corpus:\n",
        "    for word in sentence.split():\n",
        "        set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfpvIu9NgYSV",
        "outputId": "348728ee-5c29-4aac-a0d7-e99facf861f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['process', 'evolve', 'read', 'data', 'everyday', 'comprehend', 'computers', 'field', 'natural', 'make', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aGybelbFghbR",
        "outputId": "d766f4e6-b36c-4709-ee10-e5bc7b1f1c2e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'process': 0, 'evolve': 1, 'read': 2, 'data': 3, 'everyday': 4, 'comprehend': 5, 'computers': 6, 'field': 7, 'natural': 8, 'make': 9, 'language': 10}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bow_matrix = np.zeros((len(preprocessed_corpus), len(vocab)))"
      ],
      "metadata": {
        "id": "6nf0MuSmgjk_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for i, preprocessed_sentence in enumerate(preprocessed_corpus):\n",
        "    for token in preprocessed_sentence.split():\n",
        "        bow_matrix[i][position[token]] = bow_matrix[i][position[token]] + 1"
      ],
      "metadata": {
        "id": "V0NL2kPSglQD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bow_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVnic6-qgm22",
        "outputId": "aef0ccdf-9e56-4eec-e2f3-ac1eaf3463e6"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1.],\n",
              "       [1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 2.],\n",
              "       [1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "PzNa4RxUgoea"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaFjNW1fg7Kj",
        "outputId": "90315376-aeab-4858-83d8-38c72705b829"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(bow_matrix.toarray().shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_SvqLEfg9CU",
        "outputId": "780f9bea-3c25-453d-93fa-a73c513b4376"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_ngram_range = CountVectorizer(analyzer='word', ngram_range=(1,3))\n",
        "bow_matrix_ngram = vectorizer_ngram_range.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "Ct_g48P3hPHu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer_ngram_range.get_feature_names_out())\n",
        "print(bow_matrix_ngram.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df_NUsXihXyG",
        "outputId": "421ea5c3-7e00-4757-c32c-66548e7c3b33"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'comprehend language' 'comprehend language data' 'computers'\n",
            " 'computers comprehend' 'computers comprehend language' 'data' 'everyday'\n",
            " 'evolve' 'evolve everyday' 'field' 'field natural'\n",
            " 'field natural language' 'language' 'language data' 'language process'\n",
            " 'language process evolve' 'language process make' 'make' 'make computers'\n",
            " 'make computers comprehend' 'natural' 'natural language'\n",
            " 'natural language process' 'process' 'process evolve'\n",
            " 'process evolve everyday' 'process make' 'process make computers' 'read'\n",
            " 'read natural' 'read natural language']\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1]\n",
            " [1 1 1 1 1 1 1 0 0 0 0 0 0 2 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3),\n",
        "max_features = 6)\n",
        "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_max_features.get_feature_names_out())\n",
        "print(bow_matrix_max_features.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1jPQBFkhZ3A",
        "outputId": "981785e5-1ec4-4021-fd6f-bb09500d8f87"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_max_features = CountVectorizer(analyzer='word', ngram_range=(1,3), max_df\n",
        "= 3, min_df = 2)\n",
        "bow_matrix_max_features = vectorizer_max_features.fit_transform(preprocessed_corpus)\n",
        "print(vectorizer_max_features.get_feature_names_out())\n",
        "print(bow_matrix_max_features.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-6RI_6phj4O",
        "outputId": "05aa626d-4f9b-45b8-fc1b-a7d6f139b274"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[1 1 1 1 1 1]\n",
            " [2 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#TFIDF\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "9FSguXv1hzso"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftxasmENh7sA",
        "outputId": "208145ab-d9ca-47a6-84d9-a9b154cd5048"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer_l1_norm = TfidfVectorizer(norm=\"l1\")\n",
        "tf_idf_matrix_l1_norm = vectorizer_l1_norm.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "Fpfgg9OPiFyi"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer_l1_norm.get_feature_names_out())\n",
        "print(tf_idf_matrix_l1_norm.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix_l1_norm.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "buH4HGnfiNqg",
        "outputId": "371228da-09a7-43ca-b42a-890eda3487e1"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.21307663 0.         0.21307663 0.21307663 0.3607701 ]\n",
            " [0.1571718  0.1571718  0.1571718  0.         0.         0.\n",
            "  0.1856564  0.1571718  0.0928282  0.0928282  0.        ]\n",
            " [0.         0.         0.         0.2095624  0.2095624  0.2095624\n",
            "  0.12377093 0.         0.12377093 0.12377093 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer_n_gram_max_features = TfidfVectorizer(norm=\"l2\", analyzer='word', ngram_range=(1,3), max_features=6)\n",
        "tf_idf_matrix_n_gram_max_features = vectorizer_n_gram_max_features.fit_transform(preprocessed_corpus)\n",
        "\n",
        "# Get the feature names\n",
        "feature_names = vectorizer_n_gram_max_features.get_feature_names_out()\n",
        "\n",
        "# Print the top feature names\n",
        "print(\"Top feature names:\", feature_names[:6])\n",
        "\n",
        "# Print the TF-IDF matrix\n",
        "print(tf_idf_matrix_n_gram_max_features.toarray())\n",
        "\n",
        "# Print the shape of the TF-IDF matrix\n",
        "print(\"\\nThe shape of the TF-IDF matrix is:\", tf_idf_matrix_n_gram_max_features.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJGQIMGiPLl",
        "outputId": "e7e11fba-cfac-4a30-bf09-8bba74237990"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top feature names: ['language' 'language process' 'natural' 'natural language'\n",
            " 'natural language process' 'process']\n",
            "[[0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]\n",
            " [0.66666667 0.33333333 0.33333333 0.33333333 0.33333333 0.33333333]\n",
            " [0.40824829 0.40824829 0.40824829 0.40824829 0.40824829 0.40824829]]\n",
            "\n",
            "The shape of the TF-IDF matrix is: (3, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vector1, vector2):\n",
        "    vector1 = np.array(vector1)\n",
        "    vector2 = np.array(vector2)\n",
        "    return np.dot(vector1, vector2) / (np.sqrt(np.sum(vector1**2)) * np.sqrt(np.sum(vector2**2)))\n"
      ],
      "metadata": {
        "id": "gNrTiRYui-sK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "oOXE7nfajcci"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(bow_matrix.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chZBQubFjwbg",
        "outputId": "664ad471-7c9b-47ef-f03a-4fc694080255"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0 0 0 0 0 0 1 0 1 1 1]\n",
            " [1 1 1 0 0 0 2 1 1 1 0]\n",
            " [0 0 0 1 1 1 1 0 1 1 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(bow_matrix.shape[0]):\n",
        "    for j in range(i + 1, bow_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(bow_matrix.toarray()[i], bow_matrix.toarray()[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jj3yFkRTjzC1",
        "outputId": "4859b3a6-7205-48d7-8bf6-ad37772d6b71"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.6324555320336759\n",
            "The cosine similarity between the documents  0 and 2 is:  0.6123724356957946\n",
            "The cosine similarity between the documents  1 and 2 is:  0.5163977794943223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "tf_idf_matrix = vectorizer.fit_transform(preprocessed_corpus)"
      ],
      "metadata": {
        "id": "YwVpTxKLj3qS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.get_feature_names_out())\n",
        "print(tf_idf_matrix.toarray())\n",
        "print(\"\\nThe shape of the TF-IDF matrix is: \", tf_idf_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYh4L6VMj6jl",
        "outputId": "580cc79a-4372-4394-ce5a-644e439a7d4a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['comprehend' 'computers' 'data' 'everyday' 'evolve' 'field' 'language'\n",
            " 'make' 'natural' 'process' 'read']\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.41285857 0.         0.41285857 0.41285857 0.69903033]\n",
            " [0.40512186 0.40512186 0.40512186 0.         0.         0.\n",
            "  0.478543   0.40512186 0.2392715  0.2392715  0.        ]\n",
            " [0.         0.         0.         0.49711994 0.49711994 0.49711994\n",
            "  0.29360705 0.         0.29360705 0.29360705 0.        ]]\n",
            "\n",
            "The shape of the TF-IDF matrix is:  (3, 11)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(tf_idf_matrix.shape[0]):\n",
        "    for j in range(i + 1, tf_idf_matrix.shape[0]):\n",
        "        print(\"The cosine similarity between the documents \", i, \"and\", j, \"is: \",\n",
        "              cosine_similarity(tf_idf_matrix.toarray()[i], tf_idf_matrix.toarray()[j]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6T2YAAzj8Yd",
        "outputId": "b080f67d-50e1-46f7-b549-315367f30ddd"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cosine similarity between the documents  0 and 1 is:  0.39514115766749125\n",
            "The cosine similarity between the documents  0 and 2 is:  0.36365455673761865\n",
            "The cosine similarity between the documents  1 and 2 is:  0.2810071916500233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing with Lemmatization here\n",
        "preprocessed_corpus = preprocess(corpus, keep_list = [], stemming =\n",
        "False, stem_type = None,lemmatization = True, remove_stopwords = True)\n",
        "preprocessed_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lBkhwjZkBKe",
        "outputId": "e4c63892-e238-489f-eb50-5af4f9497572"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-a7d5d0d93ac1>:11: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "  cleaned_corpus = pd.Series()\n",
            "<ipython-input-12-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-12-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n",
            "<ipython-input-12-a7d5d0d93ac1>:20: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  cleaned_corpus = cleaned_corpus.append(pd.Series(' '.join(qs)))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['read natural language process',\n",
              " 'natural language process make computers comprehend language data',\n",
              " 'field natural language process evolve everyday']"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_of_words = set()\n",
        "for word in preprocessed_corpus[0].split():\n",
        "    set_of_words.add(word)\n",
        "vocab = list(set_of_words)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsmCsZCXkfxY",
        "outputId": "0323bda8-9eab-424e-dc88-8d53a3d170fe"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['process', 'natural', 'read', 'language']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position = {}\n",
        "for i, token in enumerate(vocab):\n",
        "    position[token] = i\n",
        "print(position)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVRDnf07kjZd",
        "outputId": "612519b1-6fb9-46c8-c1d8-74f26bb99b7f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'process': 0, 'natural': 1, 'read': 2, 'language': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix = np.zeros((len(preprocessed_corpus[0].split()), len(vocab)))\n",
        "one_hot_matrix.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2ul5_G6kmIC",
        "outputId": "69e2a592-4c8d-42bd-a5e3-f39ded8ff0fd"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, token in enumerate(preprocessed_corpus[0].split()):\n",
        "    one_hot_matrix[i][position[token]] = 1"
      ],
      "metadata": {
        "id": "w-pV4e3jkoJO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_matrix"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJffC2o2kqW6",
        "outputId": "3ea162e9-0c24-4c81-e119-5bcddb093cf6"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [1., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install scikit-learn\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "nO5O-9B9kshb"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading questions and answers in separate lists\n",
        "import ast\n",
        "questions = []\n",
        "answers = []\n",
        "with open('/content/drive/MyDrive/qa_Electronics.json','r') as f:\n",
        "    for line in f:\n",
        "        data = ast.literal_eval(line)\n",
        "        questions.append(data['question'].lower())\n",
        "        answers.append(data['answer'].lower())"
      ],
      "metadata": {
        "id": "TPv0meL6k4Y7"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_vec = vectorizer.fit_transform(questions)"
      ],
      "metadata": {
        "id": "j2XLmtWvk8Ww"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transform data by applying term frequency inverse document frequency (TF-IDF)\n",
        "tfidf = TfidfTransformer() #by default applies \"l2\" normalization\n",
        "X_tfidf = tfidf.fit_transform(X_vec)"
      ],
      "metadata": {
        "id": "hMbl-XLDlnke"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def conversation(im):\n",
        "    global tfidf, answers, X_tfidf\n",
        "    Y_vec = vectorizer.transform(im)\n",
        "    Y_tfidf = tfidf.fit_transform(Y_vec)\n",
        "    cos_sim = np.rad2deg(np.arccos(max(cosine_similarity(Y_tfidf, X_tfidf)[0])))\n",
        "    if cos_sim > 60 :\n",
        "        return \"sorry, I did not quite understand that\"\n",
        "    else:\n",
        "        return answers[np.argmax(cosine_similarity(Y_tfidf, X_tfidf)[0])]"
      ],
      "metadata": {
        "id": "prVCADb8lpSb"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    usr = input(\"Please enter your username: \")\n",
        "    print(\"support: Hi, welcome to Q&A support. How can I help you?\")\n",
        "    while True:\n",
        "        im = input(\"{}: \".format(usr))\n",
        "        if im.lower() == 'bye':\n",
        "            print(\"Q&A support: bye!\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Q&A support: \"+conversation([im]))\n"
      ],
      "metadata": {
        "id": "0euklrN8lrVD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyNbmHfWluCI",
        "outputId": "1c4bf57a-b3f9-4b7a-ec22-04bdb6c388c7"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please enter your username: rabinam\n",
            "support: Hi, welcome to Q&A support. How can I help you?\n",
            "rabinam: My laptop wifi is not working\n",
            "Q&A support: i believe so.\n",
            "rabinam: my laptop not start\n",
            "Q&A support: hi, you may get you laptop in 3 to 5 business day depending on you location. thanks for you interest. tech mark.\n",
            "rabinam: does that mean it covered under replacement warrenty?\n",
            "Q&A support: see this page from nikon. https://support.nikonusa.com/app/answers/detail/a_id/9919/~/glossary-of-nikkor-lens-terms\n",
            "rabinam: That does not make any sense\n",
            "Q&A support: it is a power supply and does nothing to make it a tv, just sends power to it.\n",
            "rabinam: Bye\n",
            "Q&A support: bye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ltley0_ylva7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}